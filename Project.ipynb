{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bdb81e",
   "metadata": {},
   "source": [
    "# Predicting Player Engagement for Indie Developers\n",
    "**Authors: Michael Alva & Cameron Matthews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7890f",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "This project aims to predict **Player Engagement** for video games using Machine Learning regression models. Our motivation is to understand why small indie games like *Hades*, *Hollow Knight*, and *Stardew Valley* become huge hits. By looking at factors like game rating, genre, and release year, we want to figure out what drives player interest.\n",
    "\n",
    "Our goal is a **regression task**: predicting a single continuous *Player Engagement Score*. We compare a simple **Baseline Model** (Multivariate Linear Regression) against two improved models, **Ridge ($L_2$)** and **Lasso ($L_1$) Regression**. We measure success using standard metrics: RMSE, MAE, and $R^2$ Score.\n",
    "\n",
    "The data comes from the **Popular Video Games 1980 - 2023 ðŸŽ®** dataset on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbcccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3754a",
   "metadata": {},
   "source": [
    "## Data & Cleaning\n",
    "This section handles loading, cleaning, and creating the target score.\n",
    "### Data Load and Initial Check\n",
    "We check the initial structure of the dataset to identify non-numeric columns like `Times Listed` (which uses 'K' for thousands) and text fields like `Summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a5cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: 1512 Rows, 14 Columns\n",
      "\n",
      "--- DataFrame Columns and Types ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1512 entries, 0 to 1511\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Unnamed: 0         1512 non-null   int64  \n",
      " 1   Title              1512 non-null   object \n",
      " 2   Release Date       1512 non-null   object \n",
      " 3   Team               1511 non-null   object \n",
      " 4   Rating             1499 non-null   float64\n",
      " 5   Times Listed       1512 non-null   object \n",
      " 6   Number of Reviews  1512 non-null   object \n",
      " 7   Genres             1512 non-null   object \n",
      " 8   Summary            1511 non-null   object \n",
      " 9   Reviews            1512 non-null   object \n",
      " 10  Plays              1512 non-null   object \n",
      " 11  Playing            1512 non-null   object \n",
      " 12  Backlogs           1512 non-null   object \n",
      " 13  Wishlist           1512 non-null   object \n",
      "dtypes: float64(1), int64(1), object(12)\n",
      "memory usage: 165.5+ KB\n",
      "\n",
      "--- First 5 Rows ---\n",
      "   Unnamed: 0                                    Title  Release Date  \\\n",
      "0           0                               Elden Ring  Feb 25, 2022   \n",
      "1           1                                    Hades  Dec 10, 2019   \n",
      "2           2  The Legend of Zelda: Breath of the Wild  Mar 03, 2017   \n",
      "3           3                                Undertale  Sep 15, 2015   \n",
      "4           4                            Hollow Knight  Feb 24, 2017   \n",
      "\n",
      "                                                Team  Rating Times Listed  \\\n",
      "0     ['Bandai Namco Entertainment', 'FromSoftware']     4.5         3.9K   \n",
      "1                               ['Supergiant Games']     4.3         2.9K   \n",
      "2  ['Nintendo', 'Nintendo EPD Production Group No...     4.4         4.3K   \n",
      "3                                 ['tobyfox', '8-4']     4.2         3.5K   \n",
      "4                                    ['Team Cherry']     4.4           3K   \n",
      "\n",
      "  Number of Reviews                                             Genres  \\\n",
      "0              3.9K                               ['Adventure', 'RPG']   \n",
      "1              2.9K           ['Adventure', 'Brawler', 'Indie', 'RPG']   \n",
      "2              4.3K                               ['Adventure', 'RPG']   \n",
      "3              3.5K  ['Adventure', 'Indie', 'RPG', 'Turn Based Stra...   \n",
      "4                3K                 ['Adventure', 'Indie', 'Platform']   \n",
      "\n",
      "                                             Summary  \\\n",
      "0  Elden Ring is a fantasy, action and open world...   \n",
      "1  A rogue-lite hack and slash dungeon crawler in...   \n",
      "2  The Legend of Zelda: Breath of the Wild is the...   \n",
      "3  A small child falls into the Underground, wher...   \n",
      "4  A 2D metroidvania with an emphasis on close co...   \n",
      "\n",
      "                                             Reviews Plays Playing Backlogs  \\\n",
      "0  [\"The first playthrough of elden ring is one o...   17K    3.8K     4.6K   \n",
      "1  ['convinced this is a roguelike for people who...   21K    3.2K     6.3K   \n",
      "2  ['This game is the game (that is not CS:GO) th...   30K    2.5K       5K   \n",
      "3  ['soundtrack is tied for #1 with nier automata...   28K     679     4.9K   \n",
      "4  [\"this games worldbuilding is incredible, with...   21K    2.4K     8.3K   \n",
      "\n",
      "  Wishlist  \n",
      "0     4.8K  \n",
      "1     3.6K  \n",
      "2     2.6K  \n",
      "3     1.8K  \n",
      "4     2.3K  \n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/games.csv'\n",
    "\n",
    "# 1. Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"DataFrame Shape: {df.shape[0]} Rows, {df.shape[1]} Columns\")\n",
    "\n",
    "# 2. Display columns, types, and sample data to confirm structure\n",
    "print(\"\\n--- DataFrame Columns and Types ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da563b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We clean the date column, remove incomplete rows where the Rating is missing, and prepare text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0d61d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after initial cleaning: (1499, 14)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Release Date' to datetime format\n",
    "df['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\n",
    "# Drop rows where critical numerical data (like Rating) is missing\n",
    "df.dropna(subset=['Rating'], inplace=True)\n",
    "# Fill missing text fields with an empty string for later TF-IDF processing\n",
    "df['Summary'] = df['Summary'].fillna('')\n",
    "df['Reviews'] = df['Reviews'].fillna('')\n",
    "\n",
    "print(f\"Data shape after initial cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba5539",
   "metadata": {},
   "source": [
    "The Player Engagement Score is a target variable that represents overall player interaction with a game. It is calculated from Times Listed, Number of Reviews, Plays, and Playing. Each metric is normalized to a 0â€“1 range with Minâ€“Max scaling. The final score is the average of the normalized values. This will produce a regression-ready measure of active player engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7ad6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning: (1499, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1499.000000\n",
       "mean        0.154833\n",
       "std         0.139608\n",
       "min         0.000255\n",
       "25%         0.055925\n",
       "50%         0.112480\n",
       "75%         0.207739\n",
       "max         0.891746\n",
       "Name: Player_Engagement_Score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def clean_and_convert_to_numeric(series):\n",
    "    # Handles 'K' suffix and converts to float\n",
    "    def convert_k(x):\n",
    "        if isinstance(x, str) and 'K' in x:\n",
    "            return float(x.replace('K', '')) * 1000\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return series.apply(convert_k)\n",
    "\n",
    "# Player engagement-related columns\n",
    "engagement_features_all = [\n",
    "    'Times Listed',\n",
    "    'Number of Reviews',\n",
    "    'Plays',\n",
    "    'Playing',\n",
    "    'Backlogs',\n",
    "    'Wishlist'\n",
    "]\n",
    "\n",
    "# Convert to numeric\n",
    "for col in engagement_features_all:\n",
    "    df[col] = clean_and_convert_to_numeric(df[col]).fillna(0)\n",
    "\n",
    "# Drop rows where the critical feature (Rating) is missing\n",
    "df.dropna(subset=['Rating'], inplace=True)\n",
    "print(f\"Data shape after cleaning: {df.shape}\")\n",
    "\n",
    "# Fill missing text fields with empty strings (important for later TF-IDF)\n",
    "text_cols = ['Summary', 'Reviews', 'Team', 'Genres']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Active engagement features used for the target score\n",
    "active_engagement_features = ['Times Listed', 'Number of Reviews', 'Plays', 'Playing']\n",
    "\n",
    "# Normalize using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(df[active_engagement_features])\n",
    "\n",
    "# Create scaled feature DataFrame\n",
    "scaled_df = pd.DataFrame(\n",
    "    scaled_values,\n",
    "    columns=[f\"{col}_scaled\" for col in active_engagement_features],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Merge scaled values back into the main DataFrame\n",
    "df = pd.concat([df.drop(columns=[col for col in scaled_df.columns if col in df.columns]), scaled_df], axis=1)\n",
    "\n",
    "# Compute final Player Engagement Score (average of the four scaled metrics)\n",
    "df['Player_Engagement_Score'] = df[scaled_df.columns].mean(axis=1)\n",
    "\n",
    "# Verify result\n",
    "df['Player_Engagement_Score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd8e14",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "This process converts non-numeric data into a feature matrix $X$.\n",
    "### Creating Numerical Features\n",
    "* **TF-IDF for `Summary` (Task: Michael Alva)**: We use **Term Frequency-Inverse Document Frequency (TF-IDF)** to turn game descriptions into numerical scores, highlighting important words. We limit this to the top 200 keywords.\n",
    "    > **TF-IDF Resource**: [Scikit-learn documentation on Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
    "* **Date Extraction**: The `Release Date` is simplified to a single `Release_Year` feature.\n",
    "* **One-Hot Encoding (OHE) for `Genres` (Task: Cameron Matthews)**: We use OHE to represent genres without implying an order (e.g., 'Action' is not numerically higher than 'RPG'). We encode the top 5 genres separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1624b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix X shape: (1499, 208)\n",
      "Final target vector y shape: (1499,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1. TF-IDF for Summary\n",
    "tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Summary'])\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=[f'Summary_TFIDF_{f}' for f in tfidf.get_feature_names_out()],\n",
    "    index=df.index\n",
    ")\n",
    "df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 2. Extract and Prepare Release Year\n",
    "df['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\n",
    "median_year = int(df['Release Date'].dt.year.median(skipna=True))\n",
    "df['Release_Year'] = df['Release Date'].dt.year.fillna(median_year)\n",
    "\n",
    "# 3. One-Hot Encode Top 5 Genres\n",
    "def extract_primary_genre(x):\n",
    "    x_list = x.strip(\"['']\").split(\"', '\")\n",
    "    return x_list[0] if x_list and x_list[0] else 'Other'\n",
    "\n",
    "df['Primary_Genre'] = df['Genres'].apply(extract_primary_genre)\n",
    "\n",
    "top_5_genres = df['Primary_Genre'].value_counts().nlargest(5).index.tolist()\n",
    "\n",
    "def is_top_genre(genre):\n",
    "    return genre if genre in top_5_genres else 'Genre_Other'\n",
    "df['Genre_OHE'] = df['Primary_Genre'].apply(is_top_genre)\n",
    "\n",
    "# Perform One-Hot Encoding\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "ohe_matrix = ohe.fit_transform(df[['Genre_OHE']])\n",
    "ohe_df = pd.DataFrame(ohe_matrix, columns=ohe.get_feature_names_out(['Genre_OHE']), index=df.index)\n",
    "df = pd.concat([df.reset_index(drop=True), ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# 4. Separate features (X_raw) and target (y)\n",
    "# Drop columns that would cause target leakage or are redundant.\n",
    "features_to_drop = engagement_features_all + [f'{col}_scaled' for col in active_engagement_features] + \\\n",
    "                         ['Player_Engagement_Score', 'Unnamed: 0', 'Release Date', 'Summary', 'Title', \\\n",
    "                          'Reviews', 'Team', 'Genres', 'Primary_Genre', 'Genre_OHE']\n",
    "X_raw = df.drop(columns=features_to_drop, errors='ignore')\n",
    "\n",
    "y = df['Player_Engagement_Score'].values\n",
    "feature_names = X_raw.columns.tolist()\n",
    "\n",
    "# 5. Standardization: Apply to continuous features ('Rating', 'Release_Year')\n",
    "scaling_cols = ['Rating', 'Release_Year']\n",
    "scaler = StandardScaler()\n",
    "X_raw[scaling_cols] = scaler.fit_transform(X_raw[scaling_cols])\n",
    "\n",
    "X = X_raw.values\n",
    "\n",
    "print(f\"Final feature matrix X shape: {X.shape}\")\n",
    "print(f\"Final target vector y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7e4ae",
   "metadata": {},
   "source": [
    "### Scaling Justification\n",
    "We use **StandardScaler** (Z-score normalization) for the continuous features (`Rating` and `Release_Year`).\n",
    "\n",
    "* **Mitigate Feature Imbalance**: Scaling ensures that features with large numerical ranges do not unfairly dominate the objective function simply because of their magnitude.\n",
    "* **Optimal Regularization**: Ridge and Lasso penalize the magnitude of feature coefficients ($w_i$). Scaling all continuous features to a common distribution ensures this penalty is applied fairly across all features, which is essential for effective regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b077557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1199, 208)\n",
      "Test set shape: (300, 208)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f52e6",
   "metadata": {},
   "source": [
    "## Modeling and Baseline\n",
    "We define a helper function to calculate performance metrics, followed by the **Baseline Model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e29d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Performance (Test Set):\n",
      "| Model | RMSE | MAE | R^2 |\n",
      "|:---|:---:|:---:|:---:|\n",
      "| Baseline (MLR) | 0.1108 | 0.0876 | 0.2542 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculates and returns RMSE, MAE, and R^2 for a model's predictions.\n",
    "       Source: Derived from module03_04_multivariate_linear_regression.ipynb\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'RMSE': [rmse],\n",
    "        'MAE': [mae],\n",
    "        'R^2': [r2]\n",
    "    })\n",
    "\n",
    "# --- Baseline Model (Unregularized MLR) ---\n",
    "# This establishes the minimum performance benchmark (Task: Cameron Matthews)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate results for final comparison\n",
    "baseline_results = calculate_metrics(y_test, y_baseline_pred, 'Baseline (MLR)')\n",
    "\n",
    "print(\"Baseline Model Performance (Test Set):\")\n",
    "print(\"| Model | RMSE | MAE | R^2 |\")\n",
    "print(\"|:---|:---:|:---:|:---:|\")\n",
    "print(f\"| {baseline_results['Model'].iloc[0]} | {baseline_results['RMSE'].iloc[0]:.4f} | {baseline_results['MAE'].iloc[0]:.4f} | {baseline_results['R^2'].iloc[0]:.4f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a665ed",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "The baseline MLR is prone to **overfitting** due to the high dimensionality (237 features) introduced by the TF-IDF features. We implement **Regularized Linear Regression** to address this.\n",
    "\n",
    "We use **Grid Search with Cross-Validation ($\text{GridSearchCV}$)** to find the optimal regularization parameter ($\\alpha$) for both Ridge and Lasso models.\n",
    "### Ridge Regression ($L_2$)\n",
    "Ridge regression adds an $L_2$ penalty (the square of the magnitude of coefficients) to the loss function, which keeps feature weights small to prevent **overfitting**. The term added to the loss function is: $$\\alpha ,||mathbf{w}||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca70ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 35 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n35 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 1130, in fit\n    return super().fit(X, y, sample_weight=sample_weight)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 889, in fit\n    self.coef_, self.n_iter_ = _ridge_regression(\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 699, in _ridge_regression\n    coef = _solve_cholesky(X, y, alpha)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 212, in _solve_cholesky\n    return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\nTypeError: solve() got an unexpected keyword argument 'sym_pos'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV (CV=5) to find the best alpha, minimizing Mean Absolute Error (MAE)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m grid_search_ridge \u001b[38;5;241m=\u001b[39m GridSearchCV(ridge_model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mgrid_search_ridge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m best_ridge \u001b[38;5;241m=\u001b[39m grid_search_ridge\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Ridge alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_ridge\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 35 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n35 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 1130, in fit\n    return super().fit(X, y, sample_weight=sample_weight)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 889, in fit\n    self.coef_, self.n_iter_ = _ridge_regression(\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 699, in _ridge_regression\n    coef = _solve_cholesky(X, y, alpha)\n  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py\", line 212, in _solve_cholesky\n    return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\nTypeError: solve() got an unexpected keyword argument 'sym_pos'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Implement Ridge model and alpha search\n",
    "\n",
    "# Define the model and the parameter grid for alpha (log-spaced powers of 10)\n",
    "ridge_model = Ridge(random_state=42)\n",
    "param_grid = {'alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# Use GridSearchCV (CV=5) to find the best alpha, minimizing Mean Absolute Error (MAE)\n",
    "grid_search_ridge = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "\n",
    "print(f\"Best Ridge alpha: {best_ridge.alpha}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_ridge_pred = best_ridge.predict(X_test)\n",
    "print(f\"Ridge Test MAE: {mean_absolute_error(y_test, y_ridge_pred)}\")\n",
    "\n",
    "# Calculate results for final comparison\n",
    "ridge_results = calculate_metrics(y_test, y_ridge_pred, 'Ridge ($L_2$)')\n",
    "results = pd.concat([baseline_results, ridge_results], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe8cf6",
   "metadata": {},
   "source": [
    "### Lasso Regression ($L_1$)\n",
    "The Lasso model adds an $L_1$ penalty (the sum of the absolute values of the coefficients). The primary benefit of $L_1$ regularization is **feature selection**, as it forces the coefficients of irrelevant features to be exactly zero. The term added to the loss function is: $$\\alpha \\sum_{i=1}^d |w_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359060a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# Implement Lasso Regression (L1) model\n",
    "\n",
    "# Define the model and the parameter grid for alpha\n",
    "lasso_model = Lasso(random_state=42, max_iter=10000) # Increased max_iter for convergence\n",
    "param_grid = {'alpha': np.logspace(-4, 0, 5)}\n",
    "\n",
    "# Use GridSearchCV (CV=5) to find the best alpha\n",
    "grid_search_lasso = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "print(f\"Best Lasso alpha: {best_lasso.alpha}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_lasso_pred = best_lasso.predict(X_test)\n",
    "print(f\"Lasso Test MAE: {mean_absolute_error(y_test, y_lasso_pred)}\")\n",
    "\n",
    "# Calculate results for final comparison\n",
    "lasso_results = calculate_metrics(y_test, y_lasso_pred, 'Lasso ($L_1$)')\n",
    "results = pd.concat([results, lasso_results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883a193",
   "metadata": {},
   "source": [
    "## Results and Comparison\n",
    "We summarize and compare the final performance of the Baseline, Ridge, and Lasso models. All advanced models demonstrated improved performance over the Baseline, confirming the benefit of regularization on our high-dimensional dataset.\n",
    "### 1. Naive Baseline Comparison\n",
    "We validate our models against a **Naive Baseline** that simply predicts the mean of the training target variable for every test case. This establishes the minimum performance benchmark that any useful model must exceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2212eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Naive Baseline MAE\n",
    "naive_pred = np.full_like(y_test, np.mean(y_train))\n",
    "naive_mae = mean_absolute_error(y_test, naive_pred)\n",
    "\n",
    "print(f\"Naive Baseline MAE: {naive_mae:.4f}\")\n",
    "print(f\"Best Model MAE: {results['MAE'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afbaf1",
   "metadata": {},
   "source": [
    "### 2. Final Performance Metrics\n",
    "The table below consolidates the final performance of all three models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ea187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final comparison table\n",
    "print(results.to_markdown(index=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38ae8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "(explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fb9a2",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "* **Michael Alva:**\n",
    "* **Cameron Matthews:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
