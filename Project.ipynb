{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bdb81e",
   "metadata": {},
   "source": [
    "# Predicting Player Engagement for Indie Developers\n",
    "**Authors: Michael Alva & Cameron Matthews**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7890f",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbcccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3754a",
   "metadata": {},
   "source": [
    "## Data & Cleaning\n",
    "This section loads the data, fixes missing values, prepares dates, and creates the single target score we need to predict.\n",
    "### Data Load and Initial Check\n",
    "We are using the **Popular Video Games 1980 - 2023 ðŸŽ®** dataset, available on Kaggle ([link](https://www.kaggle.com/datasets/arnabchaki/popular-video-games-1980-2023)). We load the data, check its size, and look at the types of information in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a5cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with\n",
      "DataFrame Shape: 1512 Rows, 14 Columns\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'data/games.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully with\")\n",
    "    print(f\"DataFrame Shape: {df.shape[0]} Rows, {df.shape[1]} Columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}.\")\n",
    "\n",
    "# Simpler check for column types and sample values\n",
    "# print(\"\\nColumn Data Types:\")\n",
    "# print(df.dtypes)\n",
    "# print(\"\\nFirst 5 Rows:\")\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da563b",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We clean the date column, remove incomplete rows where the Rating is missing, and prepare text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0d61d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after initial cleaning: (1499, 14)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Release Date' to datetime format\n",
    "df['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\n",
    "# Drop rows where critical numerical data (like Rating) is missing\n",
    "df.dropna(subset=['Rating'], inplace=True)\n",
    "# Fill missing text fields with an empty string for later TF-IDF processing\n",
    "df['Summary'] = df['Summary'].fillna('')\n",
    "df['Reviews'] = df['Reviews'].fillna('')\n",
    "\n",
    "print(f\"Data shape after initial cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba5539",
   "metadata": {},
   "source": [
    "The Player Engagement Score is a target variable that represents overall player interaction with a game. It is calculated from Times Listed, Number of Reviews, Plays, and Playing. Each metric is normalized to a 0â€“1 range with Minâ€“Max scaling. The final score is the average of the normalized values. This will produce a regression-ready measure of active player engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b7ad6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1499.000000\n",
       "mean        0.225641\n",
       "std         0.154465\n",
       "min         0.000000\n",
       "25%         0.099799\n",
       "50%         0.204735\n",
       "75%         0.335126\n",
       "max         0.693940\n",
       "Name: Player_Engagement_Score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cameron: Define and calculate the final Player Engagement Score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Player engagement-related columns\n",
    "engagement_features = [\n",
    "    'Times Listed',\n",
    "    'Number of Reviews',\n",
    "    'Plays',\n",
    "    'Playing'\n",
    "]\n",
    "\n",
    "# Convert to numeric\n",
    "for col in engagement_features:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill missing engagement vals with 0\n",
    "df[engagement_features] = df[engagement_features].fillna(0)\n",
    "\n",
    "# Normalize using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(df[engagement_features])\n",
    "\n",
    "# Create scaled feature DataFrame\n",
    "scaled_df = pd.DataFrame(\n",
    "    scaled_values,\n",
    "    columns=[f\"{col}_scaled\" for col in engagement_features]\n",
    ")\n",
    "\n",
    "# Merge scaled values back into the main DataFrame\n",
    "df = pd.concat([df.reset_index(drop=True), scaled_df], axis=1)\n",
    "\n",
    "# Compute final Player Engagement Score\n",
    "df['Player_Engagement_Score'] = df[[\n",
    "    'Times Listed_scaled',\n",
    "    'Number of Reviews_scaled',\n",
    "    'Plays_scaled',\n",
    "    'Playing_scaled'\n",
    "]].mean(axis=1)\n",
    "\n",
    "# Verify result\n",
    "df['Player_Engagement_Score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd8e14",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "This is the process of converting all our raw data into numbers the model can use.\n",
    "### Text Features (TF-IDF)\n",
    "We use TF-IDF to convert the game summaries into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b1624b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michael: Create TF-IDF features from text data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7e4ae",
   "metadata": {},
   "source": [
    "### Encoding and Scaling\n",
    "\n",
    "(explain why we use one-hot encoding and standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b077557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cameron: Categorical encoding and numerical scaling\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f52e6",
   "metadata": {},
   "source": [
    "## Modeling and Baseline\n",
    "We build our first simple model (the **Baseline**) to establish the minimum performance we need to beat.\n",
    "### Baseline Model (Unregularized MLR)\n",
    "We train the simplest model (Standard Multivariate Linear Regression) and calculate its performance scores (RMSE, MAE, $\\text{R}^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8e29d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cameron/Michael: Baseline Model and Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a665ed",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "(explain)\n",
    "### Ridge Regression ($L_2$)\n",
    "We use the Ridge model, which helps prevent **overfitting** by keeping the feature weights small. We will tune the alpha ($\\alpha$) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aca70ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Michael: Implement Ridge model and alpha search\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe8cf6",
   "metadata": {},
   "source": [
    "### Lasso Regression ($L_1$)\n",
    "We use the Lasso model, which (explain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359060a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cameron: Implement Lasso Regression (L1) model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883a193",
   "metadata": {},
   "source": [
    "## Results and Comparison\n",
    "We summarize and compare the final performance of the Baseline, Ridge, and Lasso models.\n",
    "### Final Analysis\n",
    "* **Task:** Present results in clear tables showing RMSE, MAE, and $\\text{R}^2$.\n",
    "* **Task:** Create plots to visually compare performance.\n",
    "* **Task:** Interpret the coefficients of the best Lasso model (feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2212eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for comparison tables and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38ae8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "(explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fb9a2",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "* **Michael Alva:**\n",
    "* **Cameron Matthews:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
